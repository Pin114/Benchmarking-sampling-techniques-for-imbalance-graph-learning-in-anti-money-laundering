import torch
import os
import sys
import numpy as np
import ast

DIR = os.path.dirname(os.path.abspath(__file__)) 
os.chdir(DIR + "/../")
sys.path.append(DIR + "/../")

# ---------------- ğŸ¯ ä¿®æ­£ 1: è™•ç† SIGKILL/å¤šæ ¸å•é¡Œ ----------------
# Node2Vecçš„åº•å±¤ä¸¦è¡Œè¨ˆç®—å¯èƒ½æœƒå°è‡´è¨˜æ†¶é«”è¶…é™ã€‚
# é€™è£¡å¼·åˆ¶å°‡ OpenMP æ ¸å¿ƒæ•¸è¨­ç‚º 1ï¼Œä»¥é™åˆ¶ä¸¦è¡Œåº¦ï¼Œæ¸›è¼• SIGKILL é¢¨éšªã€‚
os.environ["OMP_NUM_THREADS"] = "1"

# N2V_WORKERS è®Šæ•¸é›–ç„¶æ²’æœ‰åœ¨å‡½æ•¸ä¸­ä½¿ç”¨ï¼Œä½†ä¿ç•™ä½œç‚ºå–®æ ¸æ„åœ–çš„æ¨™è¨˜
N2V_WORKERS = 1 

from sklearn.ensemble import IsolationForest

from src.methods.experiments_unsupervised import *
from data.DatasetConstruction import *
from src.methods.evaluation import *
from src.methods.evaluation import random_undersample_mask # å‡è¨­é€™æ˜¯æ‚¨çš„æ¬ æ¡æ¨£å‡½æ•¸

if __name__ == "__main__":
    
    # ------------------ ğŸ¯ æ¡æ¨£æŠ€è¡“åˆ—è¡¨ (èˆ‡ train_unsupervised ä¿æŒä¸€è‡´) ------------------
    sampling_techniques = ["none", "random_undersample"] 

    use_intrinsic = True
    intrinsic_str = "_intrinsic" if use_intrinsic else "_no_intrinsic"

    if use_intrinsic:
        to_test = ["intrinsic", "positional", "deepwalk", "node2vec"]
    else:
        to_test = ["positional",  "node2vec"]

    ### Load Dataset ###
    ntw_name = "ibm"

    if ntw_name == "ibm":
        ntw = load_ibm()
    elif ntw_name == "elliptic":
        ntw = load_elliptic()
    else:
        raise ValueError("Network not found")
    
    # Masks, device and default sampling tag
    percentile_q_list = [90, 99, 99.9]
    train_mask, val_mask, test_mask = ntw.get_masks()
    # å„²å­˜åŸå§‹çš„ Train + Validation Mask
    original_train_mask = torch.logical_or(train_mask, val_mask).detach()

    device_decoder = (
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    
    # ---------- Positional ----------
    x_intrinsic = ntw.get_features_torch()

    fraud_dict = ntw.get_fraud_dict()
    fraud_dict = {k: 0 if v == 2 else v for k, v in fraud_dict.items()}


    # ------------------ ğŸ¯ è¿´åœˆé–‹å§‹ï¼šè¿­ä»£æ¡æ¨£æŠ€è¡“ ------------------
    for sampling in sampling_techniques:
        print("="*50)
        print(f"Starting test with sampling technique: {sampling.upper()}")
        print("="*50)

        samp_tag = '' if sampling == 'none' else f'_{sampling}'
        
        # æ ¹æ“šæ¡æ¨£é¡å‹æ±ºå®š train_mask_sampled
        if sampling == "none":
            # ä¸æ¡æ¨£ï¼Œä½¿ç”¨åŸå§‹çš„ Train + Val mask
            train_mask_sampled = original_train_mask
        elif sampling == "random_undersample":
            # åŸ·è¡Œæ¬ æ¡æ¨£
            # ç¢ºä¿ mask æ˜¯ä¸€å€‹æ˜ç¢ºçš„ 1D Tensorï¼Œä»¥é¿å… ValueError
            mask_to_pass = original_train_mask.contiguous().view(-1)
            
            # å‡è¨­ random_undersample_mask(ntw, mask_tensor) è¿”å›æ¡æ¨£å¾Œçš„ mask
            # evaluation.py ä¸­çš„ä¿®æ­£æ‡‰è©²ç¢ºä¿é€™è£¡è¿”å›çš„æ˜¯ PyTorch Tensor
            train_mask_sampled = random_undersample_mask(ntw, mask_to_pass)
            
        else:
             # å¦‚æœå®šç¾©äº†æ–°çš„æ¡æ¨£æŠ€è¡“ï¼Œéœ€è¦åœ¨é€™è£¡å¢åŠ è™•ç†é‚è¼¯
             raise ValueError(f"Unknown sampling technique: {sampling}. Please define its implementation.")




        skip_positional_test = False

        if "positional" in to_test:
            print("Positional features")
            param_dict = None # ç¢ºä¿ param_dict å§‹çµ‚æœ‰å®šç¾©
            
            # æª”æ¡ˆè·¯å¾‘ç¾åœ¨æœƒåŒ…å«æ¡æ¨£æ¨™ç±¤
            params_path = f"res/positional_params_{ntw_name}_unsupervised{samp_tag}.txt"
            
            if not os.path.exists(params_path):
                print(f"Warning: positional params file not found: {params_path}. Using fallback parameters.")
                # ğŸ¯ è¨­å®šå¾Œå‚™åƒæ•¸
                param_dict = {
                    "n_estimators": 100,
                    "max_samples": 0.5,
                    "max_features_dec%": 1, 
                    "bootstrap": True,
                    "alpha_pr": 0.5 # å¿…é ˆç‚º positional è¨­ç½®é è¨­ alpha_pr
                }
            else:
                with open(params_path, "r") as f:
                    params = f.readlines()
                param_dict = eval(params[0].strip())
            
            # ğŸ¯ ä¿®æ­£é»ï¼šåœ¨ try å€å¡Šä¸Šæ–¹è¨ˆç®— max_features_decï¼Œç¢ºä¿å…¶ä½œç”¨åŸŸæ­£ç¢º
            max_features_dec = param_dict.get("max_features_dec%", 1)

            # --- ğŸ¯ åŸ·è¡Œç‰¹å¾µè¨ˆç®—å’Œè©•ä¼° ---
            try:
                # ç¢ºä¿ alpha_pr å­˜åœ¨
                alpha_pr_val = param_dict.get("alpha_pr", 0.5) 
                
                features_df = positional_features_calc(
                    ntw,
                    alpha_pr=alpha_pr_val, 
                    alpha_ppr=None,
                    fraud_dict_train=None,
                    fraud_dict_test=fraud_dict,
                    ntw_name=ntw_name + "_test",
                    use_intrinsic=use_intrinsic,
                )

                # Safe column dropping
                cols_to_drop = [c for c in ["PSP", "fraud"] if c in features_df.columns]

                # ğŸ¯ ä¿®æ­£é» 1ï¼šå¼·åˆ¶ä½¿ç”¨ 'cpu' é€²è¡Œ Tensor è½‰æ›ï¼Œä»¥é¿å… MPS éŒ¯èª¤
                x = torch.tensor(features_df.drop(cols_to_drop, axis=1).values, dtype=torch.float32).to('cpu')
                y = torch.tensor(features_df["fraud"].values, dtype=torch.long).to('cpu')

                # ğŸ¯ ä¿®æ­£é» 2ï¼šç¢ºä¿ train_mask_sampled åœ¨ 'cpu' ä¸Š
                if not isinstance(train_mask_sampled, torch.Tensor):
                    train_mask_sampled = torch.tensor(train_mask_sampled, dtype=torch.bool, device='cpu')
                else:
                    train_mask_sampled = train_mask_sampled.to('cpu') # ç¢ºä¿è¨­å‚™ä¸€è‡´


                # mask including sampled train mask
                # ç¢ºä¿ test_mask ä¹Ÿåœ¨ CPU ä¸Š
                mask_s = torch.logical_or(train_mask_sampled, test_mask.to('cpu'))
            
                # ğŸ¯ ä¿®æ­£é»ï¼šè¨ˆç®— max_features
                max_features = int(np.ceil(max_features_dec * x.shape[1] / 10))

                model_pos = IsolationForest(
                    n_estimators=param_dict.get("n_estimators", 100),
                    max_samples=param_dict.get("max_samples", 0.5),
                    max_features=max_features,
                    bootstrap=param_dict.get("bootstrap", True),
                )

                AUC_list_pos, AP_list_pos, precision_dict_pos, recall_dict_pos, F1_dict_pos = evaluate_if(
                    model_pos, x[mask_s], y[mask_s], percentile_q_list=percentile_q_list
                )

                save_results_TI(AUC_list_pos, AP_list_pos, f"{ntw_name}_positional_unsupervised{intrinsic_str}{samp_tag}")
                save_results_TD(precision_dict_pos, recall_dict_pos, F1_dict_pos, f"{ntw_name}_positional_unsupervised{intrinsic_str}{samp_tag}")
            
            except Exception as e:
                print(f"[ERROR] Positional features testing failed for sampling '{sampling}'. Setting skip_positional_test=True.")
                print(f"Detailed Error: {e}")
                skip_positional_test = True # å¦‚æœç‰¹å¾µè¨ˆç®—æˆ–æ¨¡å‹è©•ä¼°å¤±æ•—ï¼Œå‰‡è¨­å®šè·³éæ¨™è¨˜

        # ğŸ¯ åœ¨é€²å…¥ DeepWalk/Node2Vec ä¹‹å‰ï¼Œæª¢æŸ¥æ˜¯å¦æ‡‰è©²è·³éæ•´å€‹å€å¡Š 
        if skip_positional_test:
            continue # è·³åˆ°ä¸‹ä¸€å€‹æ¡æ¨£è¿´åœˆ (RANDOM_UNDERSAMPLE)

        # ---------- Torch Models (DeepWalk / Node2Vec) ----------
        # ç”±æ–¼æˆ‘å€‘åœ¨è…³æœ¬é–‹é ­è¨­å®šäº† OMP_NUM_THREADS="1"ï¼Œé€™è£¡ç„¡éœ€é¡å¤–å‹•ä½œï¼Œå®ƒæœƒé™åˆ¶å¤šæ ¸é‹ç®—ã€‚
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        ntw_torch = ntw.get_network_torch().to(device)

        # Match train side slicing (remove time_step, keep 1:94)
        if hasattr(ntw_torch, "x"):
            ntw_torch.x = ntw_torch.x[:, 1:94]

        # ---------- Common Mask ----------
        # ğŸ¯ é—œéµä¿®æ­£ï¼šç¢ºä¿ train_mask_sampled æ˜¯ Tensor
        if not isinstance(train_mask_sampled, torch.Tensor):
            train_mask_sampled = torch.tensor(train_mask_sampled, dtype=torch.bool, device=ntw_torch.y.device)
            
        mask_s = torch.logical_or(train_mask_sampled, test_mask)
        # ---------- DeepWalk ----------
        if "deepwalk" in to_test:
            print("Deepwalk")
            # prefer deepwalk params file; if missing, fall back to node2vec params and force p=1,q=1
            params_path = f"res/deepwalk_params_{ntw_name}_unsupervised{samp_tag}.txt"
            used_node2vec_as_deepwalk = False
            if not os.path.exists(params_path):
                node2v_path = f"res/node2vec_params_{ntw_name}_unsupervised{samp_tag}.txt"
                if os.path.exists(node2v_path):
                    params_path = node2v_path
                    used_node2vec_as_deepwalk = True
                    print(f"Info: deepwalk params not found, falling back to node2vec params: {node2v_path}")
                else:
                    print(f"Warning: deepwalk params file not found: {params_path}. Skipping deepwalk test.")
                    params_path = None

            if params_path is not None:
                with open(params_path, "r") as f:
                    params = f.readlines()
                try:
                    param_dict = ast.literal_eval(params[0].strip())
                except Exception:
                    param_dict = eval(params[0].strip())

                    # if we used node2vec params as a deepwalk proxy, force p/q to 1
                    if used_node2vec_as_deepwalk:
                        param_dict['p'] = 1
                        param_dict['q'] = 1
                    
                    # ---  DeepWalk/Node2Vec åƒæ•¸å¾Œå‚™ ---
                    max_features_dec = param_dict.get("max_features_dec%", 1)
                    

                    model_deepwalk = node2vec_representation_torch(
                        ntw_torch,
                        train_mask_sampled,
                        test_mask,
                        # é™ä½åƒæ•¸ä»¥ç¯€çœè¨˜æ†¶é«”
                        embedding_dim=param_dict.get("embedding_dim", 16), # ç¶­æŒ 16 æˆ–é™è‡³ 8
                        walk_length=param_dict.get("walk_length", 3),      # ç”± 5 é™è‡³ 3
                        context_size=param_dict.get("context_size", 2),    # ç”± 3 é™è‡³ 2
                        walks_per_node=param_dict.get("walks_per_node", 1), # ç¶­æŒ 1
                        num_negative_samples=param_dict.get("num_negative_samples", 1), # ç¶­æŒ 1
                        p=1,
                        q=1,
                        lr=param_dict.get("lr", 0.025),
                        n_epochs=param_dict.get("n_epochs", 10),           # ç”± 20 é™è‡³ 10
                    )

                    x = model_deepwalk().detach().cpu()

                    if use_intrinsic:
                        x_intrinsic = x_intrinsic.cpu()
                        x = torch.cat((x, x_intrinsic), dim=1)[mask_s]
                    else:
                        x = x[mask_s]

                    y = ntw_torch.y.clone().detach().cpu()[mask_s]

                    # ä½¿ç”¨ä¿®æ­£å¾Œçš„ max_features_dec
                    max_features = int(np.ceil(max_features_dec * x.shape[1] / 10))

                    model_deepwalk = IsolationForest(
                        n_estimators=param_dict.get("n_estimators", 100),
                        max_samples=param_dict.get("max_samples", 0.5),
                        max_features=max_features,
                        bootstrap=param_dict.get("bootstrap", True),
                    )

                    AUC_list_dw, AP_list_dw, precision_dict_dw, recall_dict_dw, F1_dict_dw = evaluate_if(
                        model_deepwalk, x, y, percentile_q_list=percentile_q_list
                    )

                    save_results_TI(AUC_list_dw, AP_list_dw, f"{ntw_name}_deepwalk_unsupervised{intrinsic_str}{samp_tag}")
                    save_results_TD(precision_dict_dw, recall_dict_dw, F1_dict_dw, f"{ntw_name}_deepwalk_unsupervised{intrinsic_str}{samp_tag}")

            # ---------- Node2Vec ----------
            if "node2vec" in to_test:
                print("Node2vec")
                params_path = f"res/node2vec_params_{ntw_name}_unsupervised{samp_tag}.txt"
                if not os.path.exists(params_path):
                    print(f"Warning: node2vec params file not found: {params_path}. Skipping node2vec test.")
                else:
                    with open(params_path, "r") as f:
                        params = f.readlines()

                    param_dict = eval(params[0].strip())
                    
                    # --- Node2Vec åƒæ•¸å¾Œå‚™ ---
                    max_features_dec = param_dict.get("max_features_dec%", 1)

                    model_node2vec = node2vec_representation_torch(
                        ntw_torch,
                        train_mask_sampled,
                        test_mask,
                        # é™ä½åƒæ•¸ä»¥ç¯€çœè¨˜æ†¶é«”
                        embedding_dim=param_dict.get("embedding_dim", 16), # ç¶­æŒ 16 æˆ–é™è‡³ 8
                        walk_length=param_dict.get("walk_length", 3),      # ç”± 5 é™è‡³ 3
                        context_size=param_dict.get("context_size", 2),    # ç”± 3 é™è‡³ 2
                        walks_per_node=param_dict.get("walks_per_node", 1), # ç¶­æŒ 1
                        num_negative_samples=param_dict.get("num_negative_samples", 1), # ç¶­æŒ 1
                        p=param_dict.get("p", 1.0),
                        q=param_dict.get("q", 1.0),
                        lr=param_dict.get("lr", 0.025),
                        n_epochs=param_dict.get("n_epochs", 10),           # ç”± 20 é™è‡³ 10
                    )

                    x = model_node2vec().detach().cpu()

                    if use_intrinsic:
                        x_intrinsic = x_intrinsic.cpu()
                        x = torch.cat((x, x_intrinsic), dim=1)[mask_s]
                    else:
                        x = x[mask_s]

                    y = ntw_torch.y.clone().detach().cpu()[mask_s]

                    # ä½¿ç”¨ä¿®æ­£å¾Œçš„ max_features_dec
                    max_features = int(np.ceil(max_features_dec * x.shape[1] / 10))

                    model_node2vec = IsolationForest(
                        n_estimators=param_dict.get("n_estimators", 100),
                        max_samples=param_dict.get("max_samples", 0.5),
                        max_features=max_features,
                        bootstrap=param_dict.get("bootstrap", True),
                    )

                    AUC_list_n2v, AP_list_n2v, precision_dict_n2v, recall_dict_n2v, F1_dict_n2v = evaluate_if(
                        model_node2vec, x, y, percentile_q_list=percentile_q_list
                    )

                    # Build an experiment-specific filename using the key node2vec params
                    exp_name = f"{ntw_name}_node2vec"
                    exp_name += f"_p{param_dict.get('p','NA')}_q{param_dict.get('q','NA')}_d{param_dict.get('embedding_dim','NA')}_w{param_dict.get('walks_per_node','NA')}_ep{param_dict.get('n_epochs','NA')}"
                    exp_name += intrinsic_str + samp_tag

                    save_results_TI(AUC_list_n2v, AP_list_n2v, exp_name)
                    save_results_TD(precision_dict_n2v, recall_dict_n2v, F1_dict_n2v, exp_name)